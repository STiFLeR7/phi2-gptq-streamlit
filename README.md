# Phi2-GPTQ Streamlit App

A minimal Streamlit app that runs the quantized Phi-2 model (GPTQ) directly from Hugging Face and provides a simple web UI for generating responses to user prompts.

---

## ğŸš€ Live Demo

> **Deployed on Streamlit Cloud**: [https://phi2-gptq.streamlit.app](https://phi2-gptq.streamlit.app) *(replace with your actual URL)*

---

## ğŸ“¦ Model Details

* **Model**: [Phi-2 GPTQ](https://huggingface.co/STiFLeR7/Phi2-GPTQ)
* **Quantization**: 4-bit GPTQ
* **Hosted On**: Hugging Face
* **Inference**: CPU (via Streamlit Cloud)

---

## ğŸ§  Features

* Load GPTQ-quantized model from Hugging Face
* CPU-only inference using `auto-gptq`
* Clean UI with Streamlit for prompt/response interaction

---

## ğŸ—ï¸ Project Structure

```
phi2-gptq-streamlit/
â”œâ”€â”€ app.py               # Streamlit UI logic
â”œâ”€â”€ model_loader.py      # Model loading and inference logic
â””â”€â”€ requirements.txt     # Python dependencies
```

---

## â–¶ï¸ Running Locally

```bash
# Clone the repo
git clone https://github.com/STiFLeR7/phi2-gptq-streamlit.git
cd phi2-gptq-streamlit

# Create a virtual environment
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows

# Install dependencies
pip install -r requirements.txt

# Run the app
streamlit run app.py
```

---

## â˜ï¸ Deploy on Streamlit Cloud

1. Push this repo to GitHub
2. Go to [streamlit.io/cloud](https://streamlit.io/cloud)
3. Click "New App"
4. Choose this repo and select `app.py`
5. Click **Deploy**

---

## âš ï¸ Limitations

* Inference is slow due to CPU-only deployment
* Long prompts or token outputs may timeout
* For real-time performance, consider GPU-hosted APIs (we can help you set this up)

---

## ğŸ™Œ Acknowledgments

* [Phi-2 Model Authors](https://huggingface.co/microsoft/phi-2)
* [Auto-GPTQ](https://github.com/PanQiWei/AutoGPTQ)
* [Streamlit](https://streamlit.io)

---

## ğŸ‘¥ Contributors

* **Stifler** â€” Researcher & Developer | AI/ML/DL | Tech Lead at CudaBit

---

## ğŸ“œ License

MIT License
